{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib ## model persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkloc = '/home/master/Data/dummy/'\n",
    "reploc = pkloc\n",
    "loc = ''\n",
    "db_name = 'random_train.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  # create report.csv file with header (Time_taken) * rest adds auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(msg,di):               # create csv file with header\n",
    "    rep = \"\\n\"+msg+\" :\\n\\t\"+str(di)\n",
    "    fp = open(reploc+'report.txt','a')\n",
    "    fp.write(rep)\n",
    "    fp.close()\n",
    "\n",
    "def save_report_csv(msg,di):               # create csv file with header\n",
    "    cn = pd.read_csv(reploc+'report.csv')\n",
    "    for k,v in di.items():\n",
    "        cn.loc[msg,k] = v\n",
    "    #cn.to_csv(reploc+'report.csv')\n",
    "    return cn\n",
    "    \n",
    "\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(loc+db_file)\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_connection(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_rm = list( pd.read_csv('tag_to_remove.csv')['Tags'] )\n",
    "t = [str(i) for i in tag_rm]\n",
    "li_string = \"('\"+ \"','\".join(t) +\"')\" # li_string = '(\"'+ '\",\"'.join(t) +'\")' #  ' vs \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if con is not None:\n",
    "    tag_data = pd.read_sql('SELECT Tags FROM data WHERE Tags NOT IN '+li_string,con)\n",
    "else :\n",
    "    print('Conn error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tags\n",
      "0             iphone objective-c ios uiview uibutton\n",
      "1                    svg internet-explorer-9 raphael\n",
      "2  validation spring-mvc internationalization cus...\n",
      "3                             windows java copy text\n",
      "4                                  javascript jquery\n",
      "Shape (9708, 1)\n"
     ]
    }
   ],
   "source": [
    "print(tag_data.head())\n",
    "print('Shape',tag_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if con is not None:\n",
    "    t_data = pd.read_sql('SELECT Title FROM data WHERE Tags NOT IN '+li_string,con)\n",
    "else :\n",
    "    print('Conn error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title\n",
      "0       One tap triggering events on multiple views?\n",
      "1  IE9 text positioning bug when zoomed in with R...\n",
      "2  Spring MVC custom errors and internationalization\n",
      "3                How to copy text from Java program?\n",
      "4  How to scroll to a part of the page using jQuery?\n",
      "Shape (9708, 1)\n"
     ]
    }
   ],
   "source": [
    "print(t_data.head())\n",
    "print('Shape',t_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess title Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# https://stackoverflow.com/questions/35345761/python-re-split-vs-nltk-word-tokenize-and-sent-tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#t_data.head()\n",
    "#t_data.Title = t_data.Title.apply(lambda x : x.encode('utf-8'))\n",
    "t_data.Title = t_data.Title.apply(lambda x : str.lower(str(x)))\n",
    "t_data.Title = t_data.Title.apply(lambda x : re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',x))\n",
    "#title_data = t_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(sen):\n",
    "    return ' '.join(str(stemmer.stem(w)) for w in word_tokenize(sen) if w not in stop_words and (len(w)!=1 or w=='c'))\n",
    "\n",
    "#' '.join for w in word_tokenize(x) if w not in stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data.Title = t_data.Title.apply(lambda x : fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     one tap trigger event multipl view\n",
       "1    ie9 text posit bug zoom raphael svg\n",
       "2    spring mvc custom error internation\n",
       "3                 copi text java program\n",
       "4            scroll part page use jqueri\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data.Title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = t_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if con is not None:\n",
    "    b_data = pd.read_sql('SELECT Body FROM data  WHERE Tags NOT IN '+li_string,con)\n",
    "else :\n",
    "    print('Conn error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_data[:5])\n",
    "print('Shape',b_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data.Body = b_data.Body.apply(lambda x : x.encode('utf-8'))\n",
    "b_data.Body = b_data.Body.apply(lambda x : str.lower(str(x)))\n",
    "b_data.Body = b_data.Body.apply(lambda x : re.sub(r'<code>(.*?)</code>',' ',x))\n",
    "b_data.Body = b_data.Body.apply(lambda x : re.sub(r'<.*?>',' ',x))\n",
    "b_data.Body = b_data.Body.apply(lambda x : re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',x))\n",
    "\n",
    "b_data.Body = b_data.Body.apply(lambda x : fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = b_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------- title+body = ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = pd.DataFrame()\n",
    "title_data['ques'] = t_data.Title + b_data.Body\n",
    "title_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tag = CountVectorizer(tokenizer = lambda x: x.split(), binary='true',min_df = 5)  # min_df - document frequency strictly lower than the given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lab_y = vectorizer_tag.fit_transform(tag_data.Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary='true', decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function <lambda> at 0x7fb8a68a7620>, vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(tag_data.Tags,pkloc+'tag_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points : 9708\n",
      "Number of unique tags : 939\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points :\", multi_lab_y.shape[0])\n",
    "print(\"Number of unique tags :\", multi_lab_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer_tag.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose first n tags (desc order of count)\n",
    "\n",
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn = multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.2 Split the data into test and train (80:20) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9708 7766\n"
     ]
    }
   ],
   "source": [
    "tot_size = title_data.shape[0]\n",
    "train_size = int(tot_size * 0.8)\n",
    "print(tot_size,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=title_data.head(train_size)\n",
    "x_test=title_data.tail(tot_size - train_size)\n",
    "\n",
    "y_train = multi_lab_y[0:train_size,:]\n",
    "y_test = multi_lab_y[train_size:tot_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(y_test,pkloc+'y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @ Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:00.396779\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train.Title)     ## replace column name appropiately\n",
    "x_test_multilabel = vectorizer.transform(x_test.Title)\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (7766, 64691) Y : (7766, 939)\n",
      "Dimensions of test data X: (1942, 64691) Y: (1942, 939)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(x_test_multilabel,pkloc+'body_xtest_multilabel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDC with OnevsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "\n",
    "time_t = datetime.now() - start\n",
    "#print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
    "#print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
    "#print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
    "#print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_di = dict()\n",
    "rep_di[\"Time_taken\"] = str(time_t)\n",
    "rep_di[\"accuracy\"] = metrics.accuracy_score(y_test,predictions)\n",
    "rep_di[\"micro_f1_scoore\"] = metrics.f1_score(y_test, predictions, average = 'micro')\n",
    "print(str(rep_di))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_report(\"SGDC with OnevsRest - hinge\",rep_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifier,pkloc+'title_sgdc_hinge.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic with OnevsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier2 = OneVsRestClassifier(LogisticRegression(penalty='l1'), n_jobs=-1)\n",
    "classifier2.fit(x_train_multilabel, y_train)\n",
    "predictions2 = classifier2.predict(x_test_multilabel)\n",
    "\n",
    "time_t = datetime.now() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rep_di = dict()\n",
    "rep_di[\"Time_taken\"] = str(time_t)\n",
    "rep_di[\"accuracy\"] = metrics.accuracy_score(y_test,predictions2)\n",
    "rep_di[\"micro_f1_scoore\"] = metrics.f1_score(y_test, predictions2, average = 'micro')\n",
    "print(str(rep_di))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_report(\"Logistic with OnevsRest\",rep_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifier2,pkloc+'body_logistic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA  -- TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train3 =y_train.todense()\n",
    "y_test3 = y_test.todense()\n",
    "\n",
    "print type(y_train)\n",
    "print type(x_train_multilabel.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#classifier3 = OneVsRestClassifier(LDA())\n",
    "#classifier3.fit(x_train_multilabel.todense(), y_train)\n",
    "#predictions3 = classifier3.predict(x_test_multilabel)\n",
    "\n",
    "#print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "#print(\"accuracy :\",metrics.accuracy_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier4 = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "classifier4.fit(x_train_multilabel, y_train)\n",
    "predictions4 = classifier4.predict(x_test_multilabel)\n",
    "\n",
    "time_t = datetime.now() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rep_di = dict()\n",
    "rep_di[\"Time_taken\"] = str(time_t)\n",
    "rep_di[\"accuracy\"] = metrics.accuracy_score(y_test,predictions4)\n",
    "rep_di[\"micro_f1_scoore\"] = metrics.f1_score(y_test, predictions4, average = 'micro')\n",
    "print(str(rep_di))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_report(\"SVC\",rep_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifier4,pkloc+'body_svc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "classifier_k = MLkNN(k=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_k.fit(x_train_multilabel, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5 = classifier_k.predict(x_test_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_di = dict()\n",
    "rep_di[\"Time_taken\"] = str(time_t)\n",
    "rep_di[\"accuracy\"] = metrics.accuracy_score(y_test,predictions5)\n",
    "rep_di[\"micro_f1_scoore\"] = metrics.f1_score(y_test, predictions5, average = 'micro')\n",
    "print(str(rep_di))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_report(\"MLkNN 21\",rep_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = vectorizer_tag.inverse_transform(y_test)\n",
    "actual = [' '.join(i) for i in actual]\n",
    "actual = pd.Series(actual,name=\"Actual\")\n",
    "#actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vectorizer_tag.inverse_transform(predictions)\n",
    "preds = [' '.join(i) for i in preds]\n",
    "preds = pd.Series(preds,name=\"Pred_by_ques\")\n",
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resul_df = pd.concat([actual,preds],axis=1)\n",
    "#resul_df.to_csv(\"01_resul_rep.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rr = pd.read_csv('01_resul_rep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rr.append?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr['pred_by_ques']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.to_csv(\"01_rep_mindf_03.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv('01_rep_mindf_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_x_test_multilabel = joblib.load('/data/title_xtest_multilabel.pkl')\n",
    "\n",
    "title_sgc = joblib.load('/data/title_sgdc.pkl')\n",
    "\n",
    "pred_title = title_sgc.predict(title_x_test_multilabel)\n",
    "\n",
    "print(\"From title accuracy :\",metrics.accuracy_score(y_test,pred_title))\n",
    "print(\"From title micro f1 scoore :\",metrics.f1_score(y_test, pred_title, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_x_test_multilabel = joblib.load('/data/body_xtest_multilabel.pkl')\n",
    "\n",
    "body_sgc = joblib.load('/data/body_sgdc.pkl')\n",
    "\n",
    "pred_body = body_sgc.predict(body_x_test_multilabel)\n",
    "\n",
    "print(\"From body accuracy :\",metrics.accuracy_score(y_test,pred_body))\n",
    "print(\"From body micro f1 scoore :\",metrics.f1_score(y_test, pred_body, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_or = pred_title.todense() | pred_body.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From OR accuracy :\",metrics.accuracy_score(y_test,p_or))\n",
    "print(\"From OR micro f1 scoore :\",metrics.f1_score(y_test, p_or, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vectorizer_tag.inverse_transform(p_or)\n",
    "preds = [' '.join(i) for i in preds]\n",
    "preds = pd.Series(preds,name=\"Pred_by_OR\")\n",
    "r['pred_by_OR'] = preds\n",
    "r.to_csv(\"01_rep_mindf_03.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv('01_rep_mindf_03.csv')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
